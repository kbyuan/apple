{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EC219_project5_bk_10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "avU9SzDp5tn1",
        "colab_type": "code",
        "outputId": "4f9559b1-041e-41da-9082-08a3e8abdc77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dMkkIp46ysY",
        "colab_type": "code",
        "outputId": "c0a46e3f-84da-4b70-f094-19e3ef0c0fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls gdrive/'My Drive'/EE219/project5/tweet_data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Project5_Readme.txt\ttweets_#nfl.txt       tweets_#superbowl.txt\n",
            "tweets_#gohawks.txt\ttweets_#patriots.txt\n",
            "tweets_#gopatriots.txt\ttweets_#sb49.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbq9S7O4SCb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "\n",
        "import datetime, time\n",
        "import pytz\n",
        "import collections\n",
        "X_data = [] \n",
        "Y_data = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxrf7w5R8BVt",
        "colab_type": "code",
        "outputId": "45a29658-e891-40bd-afa6-edb053b4e344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "##Beofre 8AM\n",
        "hashtags = ['#gohawks', '#gopatriots', '#nfl', '#patriots', '#sb49', '#superbowl']\n",
        "#hashtags = ['#gohawks']\n",
        "my_list = []\n",
        "X = {}\n",
        "num_tweets = {}\n",
        "num_followers = 0\n",
        "num_retweets = 0\n",
        "max_followers = {}\n",
        "total_tweets = 0\n",
        "total_retweets = {}\n",
        "sum_followers = {}\n",
        "regression_result = []\n",
        "user_mentions = {}\n",
        "impressions_count = {}\n",
        "author_names = {}\n",
        "\n",
        "url_count = {}\n",
        "fav_count = {}\n",
        "count = 0\n",
        "pst_tz = pytz.timezone('America/Los_Angeles')\n",
        "X.clear()\n",
        "X_data.clear()\n",
        "Y_data.clear()\n",
        "for h in hashtags:\n",
        "  print(\"+++++++++++++++++++++++++++++++++\")\n",
        "  print(\"### Working on hashtag \",h)\n",
        "  print(\"+++++++++++++++++++++++++++++++++\")   \n",
        "  with open('gdrive/My Drive/EE219/project5/tweet_data/tweets_' + h + '.txt', 'r', encoding=\"utf8\") as file:\n",
        "      for line in file:\n",
        "          parsed_obj = json.loads(line)\n",
        "          #print(parsed_obj)\n",
        "          citation_time = parsed_obj['citation_date']\n",
        "            #creating a copy \n",
        "          citation_time_hr = citation_time \n",
        "          citation_time_hr -= citation_time_hr % 3600\n",
        "          time_inst_hr = citation_time_hr\n",
        "          \n",
        "         # time_inst = datetime.datetime.fromtimestamp(unix_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          #time_inst_hr = datetime.datetime.fromtimestamp(citation_time, pst_tz).strftime('%H')\n",
        "          \n",
        "          # Time given in the problem is in PST. Dont know how to invoke the pst format to get the unix_time. So adding +8 into it. \n",
        "          feb1st8am_time = datetime.datetime(2015, 2, 1, 16, 0).timestamp()\n",
        "          #feb1st8pm_time = datetime.datetime(2015, 2, 2, 4, 0).timestamp()\n",
        "        #  X['time_inst_hr'].append()\n",
        "         # print(feb1st8am_time)\n",
        "         # print(citation_time)\n",
        "       \n",
        "          if citation_time < feb1st8am_time: \n",
        "              #print(\"working on citation_time=\",citation_time)\n",
        "              first_post = parsed_obj['firstpost_date']\n",
        "              count += 1\n",
        "              num_followers = parsed_obj['author']['followers']\n",
        "              author_name = parsed_obj['author']['name']\n",
        "              num_retweets = parsed_obj['metrics']['citations']['total']\n",
        "              total_tweets = 1 \n",
        "              mentions = len(parsed_obj['tweet']['entities']['user_mentions'])\n",
        "              urls = len(parsed_obj['tweet']['entities']['urls'])\n",
        "              favorite_count = parsed_obj['tweet']['favorite_count']\n",
        "              impressions_num = parsed_obj['metrics']['impressions']\n",
        "              author_name = parsed_obj['author']['name']\n",
        "\n",
        "              if(time_inst_hr in num_tweets ):\n",
        "                  num_tweets[time_inst_hr] += total_tweets\n",
        "              else:\n",
        "                 num_tweets[time_inst_hr] = total_tweets  \n",
        "\n",
        "              if(time_inst_hr in total_retweets ):\n",
        "                  total_retweets[time_inst_hr] += num_retweets\n",
        "              else:\n",
        "                 total_retweets[time_inst_hr] = num_retweets \n",
        "\n",
        "              if(time_inst_hr in sum_followers):\n",
        "                  sum_followers[time_inst_hr] += num_followers\n",
        "              else:\n",
        "                 sum_followers[time_inst_hr] = num_followers     \n",
        "\n",
        "              if(time_inst_hr in max_followers):\n",
        "                if num_followers > max_followers[time_inst_hr]:\n",
        "                  max_followers[time_inst_hr] = num_followers\n",
        "              else:\n",
        "                 max_followers[time_inst_hr] = num_followers  \n",
        "\n",
        "              if(time_inst_hr in user_mentions):\n",
        "                  user_mentions[time_inst_hr] += mentions\n",
        "              else:\n",
        "                 user_mentions[time_inst_hr] = mentions\n",
        "\n",
        "              if(time_inst_hr in url_count):\n",
        "                  url_count[time_inst_hr] += urls\n",
        "              else:\n",
        "                 url_count[time_inst_hr] = urls   \n",
        "\n",
        "              if(time_inst_hr in fav_count):\n",
        "                  fav_count[time_inst_hr] += favorite_count\n",
        "              else:\n",
        "                 fav_count[time_inst_hr] = favorite_count   \n",
        "\n",
        "              if(time_inst_hr in impressions_count):\n",
        "                  impressions_count[time_inst_hr] += impressions_num\n",
        "              else:\n",
        "                 impressions_count[time_inst_hr] = impressions_num \n",
        "\n",
        "              if(time_inst_hr in author_names):\n",
        "                  author_names[time_inst_hr].append(author_name)\n",
        "              else:\n",
        "                 author_names[time_inst_hr] = [author_name]    \n",
        "          \n",
        "              # reset the values to 0 for next iteration\n",
        "              num_followers = 0\n",
        "              num_retweets  = 0\n",
        "              total_tweets  = 0\n",
        "          else: \n",
        "              #print(\"skipped citation_time\",citation_time)\n",
        "              num_followers = 0\n",
        "              num_retweets  = 0\n",
        "              total_tweets  = 0\n",
        "            \n",
        "          #if count == 100:\n",
        "          #  break\n",
        "# print(author_names)\n",
        "X_data = [] \n",
        "Y_data = []\n",
        "#just to get the keys    \n",
        "for key in sorted (num_tweets.keys()):\n",
        "    #print(key)\n",
        "    #value = int(key)\n",
        "    time_of_day = int(int(datetime.datetime.fromtimestamp(key).strftime('%H')))\n",
        "    day_value = int(datetime.datetime.fromtimestamp(key).strftime('%w'))\n",
        "    #authors_list = author_names[key]\n",
        "    unique_authors_count = len(set(author_names[key]))\n",
        "    #print(\"authrs list and count:\",authors_list,unique_authors_count)\n",
        "    X[key] = [num_tweets[key],total_retweets[key],sum_followers[key],max_followers[key],user_mentions[key],url_count[key],fav_count[key],impressions_count[key],unique_authors_count]\n",
        "    # Num_tweets, total_retweets, sum of followers , max_followers , time of tweet , user mentions, url_count,fav_count,impressions, unique_authors_count\n",
        "    X_data.append([num_tweets[key],total_retweets[key],sum_followers[key],max_followers[key],time_of_day,user_mentions[key],url_count[key],fav_count[key],impressions_count[key],unique_authors_count,day_value])\n",
        "    Y_data.append([num_tweets[key]])\n",
        "#print(X_data)\n",
        "#print(Y_data)\n",
        "Y = collections.deque(Y_data)\n",
        "#print(Y)\n",
        "Y.rotate(-1)\n",
        "#print(Y)\n",
        "Y = list(Y)\n",
        "#print(Y)\n",
        "\n",
        "\n",
        "Y = np.ravel(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #gohawks\n",
            "+++++++++++++++++++++++++++++++++\n",
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #gopatriots\n",
            "+++++++++++++++++++++++++++++++++\n",
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #nfl\n",
            "+++++++++++++++++++++++++++++++++\n",
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #patriots\n",
            "+++++++++++++++++++++++++++++++++\n",
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #sb49\n",
            "+++++++++++++++++++++++++++++++++\n",
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #superbowl\n",
            "+++++++++++++++++++++++++++++++++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n51yQyxj_C6u",
        "colab_type": "code",
        "outputId": "4d225ae0-3df8-4cd6-c387-c39c42ac0bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'min_samples_leaf': [1, 2, 4],\n",
        "'min_samples_split': [2, 5, 10],\n",
        "'n_estimators': [200, 400, 600, 800, 1000,\n",
        "1200, 1400, 1600, 1800, 2000]\n",
        "}\n",
        "# Create a based model\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb  = GradientBoostingRegressor(random_state=0)\n",
        "\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = gb, param_grid = param_grid, \n",
        "                         cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
        "\n",
        "grid_search.fit(X_data, Y)\n",
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 60,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 2,\n",
              " 'min_samples_split': 5,\n",
              " 'n_estimators': 400}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqPf9J6iJiem",
        "colab_type": "code",
        "outputId": "dadd3f8a-dff0-4ee7-cb29-860f63d475a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#def evaluate(model, test_features, test_labels):\n",
        "    #predictions = model.predict(test_features)\n",
        "    #errors = abs(predictions - test_labels)\n",
        "    #mape = 100 * np.mean(errors / test_labels)\n",
        "    #accuracy = 100 - mape\n",
        "    #print('Model Performance')\n",
        "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    #print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    #return accuracy\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "best_grid = grid_search.best_estimator_\n",
        "print(best_grid)\n",
        "#grid_accuracy = evaluate(best_grid, X_test, y_test)\n",
        "\n",
        "\n",
        "gb =  GradientBoostingRegressor(random_state=0, n_estimators=400, max_depth=60, max_features='sqrt', \n",
        "                                  min_samples_leaf=2, min_samples_split=5)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Mean Square Error\", metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean-Square Error\" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "# print(\"Accuracy \", metrics.accuracy_score(Y, Y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
            "                          learning_rate=0.1, loss='ls', max_depth=60,\n",
            "                          max_features='sqrt', max_leaf_nodes=None,\n",
            "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                          min_samples_leaf=2, min_samples_split=5,\n",
            "                          min_weight_fraction_leaf=0.0, n_estimators=400,\n",
            "                          n_iter_no_change=None, presort='auto', random_state=0,\n",
            "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
            "                          verbose=0, warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Square Error 60095.710113543275\n",
            "Root Mean-Square Error 245.14426388056336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyUepFi5jam0",
        "colab_type": "code",
        "outputId": "08d1f07c-3337-4eb3-d9c7-26c86689994d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#During Superbowl\n",
        "\n",
        "hashtags = ['#gohawks', '#gopatriots', '#nfl', '#patriots', '#sb49', '#superbowl']\n",
        "#hashtags = ['#gohawks']\n",
        "my_list = []\n",
        "X = {}\n",
        "num_tweets = {}\n",
        "num_followers = 0\n",
        "num_retweets = 0\n",
        "max_followers = {}\n",
        "total_tweets = 0\n",
        "total_retweets = {}\n",
        "sum_followers = {}\n",
        "regression_result = []\n",
        "user_mentions = {}\n",
        "impressions_count = {}\n",
        "author_names = {}\n",
        "\n",
        "url_count = {}\n",
        "fav_count = {}\n",
        "count = 0\n",
        "pst_tz = pytz.timezone('America/Los_Angeles')\n",
        "X.clear()\n",
        "X_data.clear()\n",
        "Y_data.clear()\n",
        "for h in hashtags:\n",
        "  print(\"+++++++++++++++++++++++++++++++++\")\n",
        "  print(\"### Working on hashtag \",h)\n",
        "  print(\"+++++++++++++++++++++++++++++++++\")   \n",
        "  with open('gdrive/My Drive/EE219/project5/tweet_data/tweets_' + h + '.txt', 'r', encoding=\"utf8\") as file:\n",
        "      for line in file:\n",
        "          parsed_obj = json.loads(line)\n",
        "          #print(parsed_obj)\n",
        "          citation_time = parsed_obj['citation_date']\n",
        "          time_inst = datetime.datetime.fromtimestamp(citation_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          time_inst_hr = datetime.datetime.fromtimestamp(citation_time, pst_tz).strftime('%H')\n",
        "          #Time Zone given in the problem are in PST format. So added +8 below 2 lines as I dont know how to pick the PST time here. \n",
        "          feb1st8pm_time = datetime.datetime(2015, 2, 2, 4, 0).timestamp()\n",
        "          feb1st8am_time = datetime.datetime(2015, 2, 1, 16, 0).timestamp()\n",
        "          \n",
        "          #print(\"feb1st\",feb1st8am_time)\n",
        "          #print(\"citation\",citation_time)\n",
        "          #print(\"time_inst\", time_inst)\n",
        "         \n",
        "          \n",
        "          #print(\"citation_rounded\",citation_time  )\n",
        "          #time_inst = datetime.datetime.fromtimestamp(citation_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          #print(\"time_inst\", time_inst)\n",
        "          #feb1st8am_time = feb1st8am_time + 300\n",
        "          keys = np.arange(feb1st8am_time,feb1st8pm_time,300)\n",
        "          #print(keys)\n",
        "          #print(len(keys))\n",
        "          time_inst_8am = datetime.datetime.fromtimestamp(feb1st8am_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          time_inst_8pm = datetime.datetime.fromtimestamp(feb1st8pm_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          #print(\"8am =\",time_inst_8am)\n",
        "          #print(\"8pm =\",time_inst_8pm)\n",
        "        #  X['time_inst_hr'].append()\n",
        "         # print(feb1st8am_time)\n",
        "         # print(citation_time)\n",
        "          count += 1\n",
        "          if  citation_time >= feb1st8am_time and citation_time < feb1st8pm_time:\n",
        "              #print(\"working on citation_time=\",citation_time)\n",
        "              time_inst = datetime.datetime.fromtimestamp(citation_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "              #print(\"time_inst\", time_inst)\n",
        "              first_post = parsed_obj['firstpost_date']\n",
        "              # Round down to  5 minute value\n",
        "              citation_time -= citation_time % 300\n",
        "              # Key value is the rounded citation time. \n",
        "              time_inst_5m = citation_time\n",
        "              num_followers = parsed_obj['author']['followers']\n",
        "              author_name = parsed_obj['author']['name']\n",
        "              num_retweets = parsed_obj['metrics']['citations']['total']\n",
        "              total_tweets = 1 \n",
        "              mentions = len(parsed_obj['tweet']['entities']['user_mentions'])\n",
        "              urls = len(parsed_obj['tweet']['entities']['urls'])\n",
        "              favorite_count = parsed_obj['tweet']['favorite_count']\n",
        "              impressions_num = parsed_obj['metrics']['impressions']\n",
        "              author_name = parsed_obj['author']['name']\n",
        "              if(time_inst_5m in num_tweets ):\n",
        "                  num_tweets[time_inst_5m] += total_tweets\n",
        "              else:\n",
        "                 num_tweets[time_inst_5m] = total_tweets  \n",
        "\n",
        "              if(time_inst_5m in total_retweets ):\n",
        "                  total_retweets[time_inst_5m] += num_retweets\n",
        "              else:\n",
        "                 total_retweets[time_inst_5m] = num_retweets \n",
        "\n",
        "              if(time_inst_hr in sum_followers):\n",
        "                  sum_followers[time_inst_5m] += num_followers\n",
        "              else:\n",
        "                 sum_followers[time_inst_5m] = num_followers     \n",
        "\n",
        "              if(time_inst_5m in max_followers):\n",
        "                if num_followers > max_followers[time_inst_5m]:\n",
        "                  max_followers[time_inst_5m] = num_followers\n",
        "              else:\n",
        "                 max_followers[time_inst_5m] = num_followers  \n",
        "\n",
        "              if(time_inst_5m in user_mentions):\n",
        "                  user_mentions[time_inst_5m] += mentions\n",
        "              else:\n",
        "                 user_mentions[time_inst_5m] = mentions\n",
        "\n",
        "              if(time_inst_5m in url_count):\n",
        "                  url_count[time_inst_5m] += urls\n",
        "              else:\n",
        "                 url_count[time_inst_5m] = urls   \n",
        "\n",
        "              if(time_inst_5m in fav_count):\n",
        "                  fav_count[time_inst_5m] += favorite_count\n",
        "              else:\n",
        "                 fav_count[time_inst_5m] = favorite_count   \n",
        "\n",
        "              if(time_inst_5m in impressions_count):\n",
        "                  impressions_count[time_inst_5m] += impressions_num\n",
        "              else:\n",
        "                 impressions_count[time_inst_5m] = impressions_num \n",
        "\n",
        "              if(time_inst_5m in author_names):\n",
        "                  author_names[time_inst_5m].append(author_name)\n",
        "              else:\n",
        "                 author_names[time_inst_5m] = [author_name]    \n",
        "          \n",
        "              # reset the values to 0 for next iteration\n",
        "              num_followers = 0\n",
        "              num_retweets  = 0\n",
        "              total_tweets  = 0\n",
        "          else: \n",
        "              #print(\"skipped citation_time\",citation_time)\n",
        "              num_followers = 0\n",
        "              num_retweets  = 0\n",
        "              total_tweets  = 0\n",
        "          #   print (count)\n",
        "          #if count == 5:\n",
        "          #    break\n",
        "          \n",
        "      #print(author_names)\n",
        "    #  X_data = [] \n",
        "    #  Y_data = []\n",
        "      #just to get the keys  \n",
        "    \n",
        "for key in sorted (num_tweets.keys()):\n",
        "    #print(key)\n",
        "    #value = int(key)\n",
        "    time_of_day = int(int(datetime.datetime.fromtimestamp(key).strftime('%H')))\n",
        "    #authors_list = author_names[key]\n",
        "    unique_authors_count = len(set(author_names[key]))\n",
        "    day_value = int(datetime.datetime.fromtimestamp(key).strftime('%w'))\n",
        "    #print(\"authrs list and count:\",authors_list,unique_authors_count)\n",
        "    X[key] = [num_tweets[key],total_retweets[key],sum_followers[key],max_followers[key],user_mentions[key],url_count[key],fav_count[key],impressions_count[key],unique_authors_count]\n",
        "    # Num_tweets, total_retweets, sum of followers , max_followers , time of tweet , user mentions, url_count,fav_count,impressions, unique_authors_count\n",
        "    X_data.append([num_tweets[key],total_retweets[key],sum_followers[key],max_followers[key],time_of_day,user_mentions[key],url_count[key],fav_count[key],impressions_count[key],unique_authors_count,day_value])\n",
        "    Y_data.append([num_tweets[key]])\n",
        "    \n",
        "#print(X_data)\n",
        "#print(Y_data, len(Y_data))\n",
        "Y = collections.deque(Y_data)\n",
        "#print(Y)\n",
        "Y.rotate(-1)\n",
        "# print(Y)\n",
        "Y = list(Y)\n",
        "#print(Y)\n",
        "      \n",
        "\n",
        "Y = np.ravel(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++++++++++++++++++++++++++++++++\n",
            "### Working on hashtag  #gohawks\n",
            "+++++++++++++++++++++++++++++++++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHWAIHdgGXjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'min_samples_leaf': [1, 2, 4],\n",
        "'min_samples_split': [2, 5, 10],\n",
        "'n_estimators': [200, 400, 600, 800, 1000,\n",
        "1200, 1400, 1600, 1800, 2000]\n",
        "}\n",
        "# Create a based model\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb  = GradientBoostingRegressor(random_state=0)\n",
        "\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = gb, param_grid = param_grid, \n",
        "                         cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
        "\n",
        "grid_search.fit(X_data, Y)\n",
        "grid_search.best_params_\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHG0nK-CGgMb",
        "colab_type": "code",
        "outputId": "8528c1fc-774b-450c-f956-4b6b5eb20d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#def evaluate(model, test_features, test_labels):\n",
        "    #predictions = model.predict(test_features)\n",
        "    #errors = abs(predictions - test_labels)\n",
        "    #mape = 100 * np.mean(errors / test_labels)\n",
        "    #accuracy = 100 - mape\n",
        "    #print('Model Performance')\n",
        "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    #print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    #return accuracy\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "best_grid = grid_search.best_estimator_\n",
        "print(best_grid)\n",
        "#grid_accuracy = evaluate(best_grid, X_test, y_test)\n",
        "\n",
        "gb =  GradientBoostingRegressor(random_state=0, n_estimators=200, max_depth=20, max_features='sqrt', \n",
        "                                  min_samples_leaf=1, min_samples_split=2)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Mean Square Error\", metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean-Square Error\" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "# print(\"Accuracy \", metrics.accuracy_score(Y, Y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
            "                          learning_rate=0.1, loss='ls', max_depth=20,\n",
            "                          max_features='sqrt', max_leaf_nodes=None,\n",
            "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                          min_samples_leaf=1, min_samples_split=2,\n",
            "                          min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                          n_iter_no_change=None, presort='auto', random_state=0,\n",
            "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
            "                          verbose=0, warm_start=False)\n",
            "Mean Square Error 1322530.1697559783\n",
            "Root Mean-Square Error 1150.0131172103988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei0TDmXXGpRD",
        "colab_type": "code",
        "outputId": "09023aed-a82d-4579-f874-0cd1876f0e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "hashtags = ['#gohawks', '#gopatriots', '#nfl', '#patriots', '#sb49', '#superbowl']\n",
        "#hashtags = ['#gohawks']\n",
        "my_list = []\n",
        "X = {}\n",
        "num_tweets = {}\n",
        "num_followers = 0\n",
        "num_retweets = 0\n",
        "max_followers = {}\n",
        "total_tweets = 0\n",
        "total_retweets = {}\n",
        "sum_followers = {}\n",
        "regression_result = []\n",
        "user_mentions = {}\n",
        "impressions_count = {}\n",
        "author_names = {}\n",
        "\n",
        "url_count = {}\n",
        "fav_count = {}\n",
        "count = 0\n",
        "pst_tz = pytz.timezone('America/Los_Angeles')\n",
        "X.clear()\n",
        "X_data.clear()\n",
        "Y_data.clear()\n",
        "for h in hashtags:\n",
        "  print(\"+++++++++++++++++++++++++++++++++\")\n",
        "  print(\"### Working on hashtag \",h)\n",
        "  print(\"+++++++++++++++++++++++++++++++++\")   \n",
        "  with open('gdrive/My Drive/EE219/project5/tweet_data/tweets_' + h + '.txt', 'r', encoding=\"utf8\") as file:\n",
        "      for line in file:\n",
        "          parsed_obj = json.loads(line)\n",
        "          #print(parsed_obj)\n",
        "          citation_time = parsed_obj['citation_date']\n",
        "               #creating a copy \n",
        "          citation_time_hr = citation_time \n",
        "          citation_time_hr -= citation_time_hr % 3600\n",
        "          time_inst_hr = citation_time_hr\n",
        "         # time_inst = datetime.datetime.fromtimestamp(unix_time, pst_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          #time_inst_hr = datetime.datetime.fromtimestamp(citation_time, pst_tz).strftime('%H')\n",
        "          \n",
        "          # Time given in the problem is in PST. Dont know how to invoke the pst format to get the unix_time. So adding +8 into it. \n",
        "          #feb1st8am_time = datetime.datetime(2015, 2, 1, 16, 0).timestamp()\n",
        "          feb1st8pm_time = datetime.datetime(2015, 2, 2, 4, 0).timestamp()\n",
        "        #  X['time_inst_hr'].append()\n",
        "         # print(feb1st8am_time)\n",
        "         # print(citation_time)\n",
        "       \n",
        "          if citation_time >= feb1st8pm_time: \n",
        "              #print(\"working on citation_time=\",citation_time)\n",
        "              first_post = parsed_obj['firstpost_date']\n",
        "              count += 1\n",
        "              num_followers = parsed_obj['author']['followers']\n",
        "              author_name = parsed_obj['author']['name']\n",
        "              num_retweets = parsed_obj['metrics']['citations']['total']\n",
        "              total_tweets = 1 \n",
        "              mentions = len(parsed_obj['tweet']['entities']['user_mentions'])\n",
        "              urls = len(parsed_obj['tweet']['entities']['urls'])\n",
        "              favorite_count = parsed_obj['tweet']['favorite_count']\n",
        "              impressions_num = parsed_obj['metrics']['impressions']\n",
        "              author_name = parsed_obj['author']['name']\n",
        "\n",
        "              if(time_inst_hr in num_tweets ):\n",
        "                  num_tweets[time_inst_hr] += total_tweets\n",
        "              else:\n",
        "                 num_tweets[time_inst_hr] = total_tweets  \n",
        "\n",
        "              if(time_inst_hr in total_retweets ):\n",
        "                  total_retweets[time_inst_hr] += num_retweets\n",
        "              else:\n",
        "                 total_retweets[time_inst_hr] = num_retweets \n",
        "\n",
        "              if(time_inst_hr in sum_followers):\n",
        "                  sum_followers[time_inst_hr] += num_followers\n",
        "              else:\n",
        "                 sum_followers[time_inst_hr] = num_followers     \n",
        "\n",
        "              if(time_inst_hr in max_followers):\n",
        "                if num_followers > max_followers[time_inst_hr]:\n",
        "                  max_followers[time_inst_hr] = num_followers\n",
        "              else:\n",
        "                 max_followers[time_inst_hr] = num_followers  \n",
        "\n",
        "              if(time_inst_hr in user_mentions):\n",
        "                  user_mentions[time_inst_hr] += mentions\n",
        "              else:\n",
        "                 user_mentions[time_inst_hr] = mentions\n",
        "\n",
        "              if(time_inst_hr in url_count):\n",
        "                  url_count[time_inst_hr] += urls\n",
        "              else:\n",
        "                 url_count[time_inst_hr] = urls   \n",
        "\n",
        "              if(time_inst_hr in fav_count):\n",
        "                  fav_count[time_inst_hr] += favorite_count\n",
        "              else:\n",
        "                 fav_count[time_inst_hr] = favorite_count   \n",
        "\n",
        "              if(time_inst_hr in impressions_count):\n",
        "                  impressions_count[time_inst_hr] += impressions_num\n",
        "              else:\n",
        "                 impressions_count[time_inst_hr] = impressions_num \n",
        "\n",
        "              if(time_inst_hr in author_names):\n",
        "                  author_names[time_inst_hr].append(author_name)\n",
        "              else:\n",
        "                 author_names[time_inst_hr] = [author_name]    \n",
        "          \n",
        "              # reset the values to 0 for next iteration\n",
        "              num_followers = 0\n",
        "              num_retweets  = 0\n",
        "              total_tweets  = 0\n",
        "          else: \n",
        "              #print(\"skipped citation_time\",citation_time)\n",
        "              num_followers = 0\n",
        "              num_retweets  = 0\n",
        "              total_tweets  = 0\n",
        "            \n",
        "          #if count == 100:\n",
        "          #  break\n",
        "# print(author_names)\n",
        "X_data = [] \n",
        "Y_data = []\n",
        "#just to get the keys    \n",
        "for key in sorted (num_tweets.keys()):\n",
        "    #print(key)\n",
        "    #value = int(key)\n",
        "    time_of_day = int(int(datetime.datetime.fromtimestamp(key).strftime('%H')))\n",
        "    #authors_list = author_names[key]\n",
        "    unique_authors_count = len(set(author_names[key]))\n",
        "    day_value = int(datetime.datetime.fromtimestamp(key).strftime('%w'))\n",
        "    #print(\"authrs list and count:\",authors_list,unique_authors_count)\n",
        "    X[key] = [num_tweets[key],total_retweets[key],sum_followers[key],max_followers[key],user_mentions[key],url_count[key],fav_count[key],impressions_count[key],unique_authors_count]\n",
        "    # Num_tweets, total_retweets, sum of followers , max_followers , time of tweet , user mentions, url_count,fav_count,impressions, unique_authors_count\n",
        "    X_data.append([num_tweets[key],total_retweets[key],sum_followers[key],max_followers[key],time_of_day,user_mentions[key],url_count[key],fav_count[key],impressions_count[key],unique_authors_count,day_value])\n",
        "    Y_data.append([num_tweets[key]])\n",
        "#print(X_data)\n",
        "#print(Y_data)\n",
        "Y = collections.deque(Y_data)\n",
        "#print(Y)\n",
        "Y.rotate(-1)\n",
        "#print(Y)\n",
        "Y = list(Y)\n",
        "#print(Y)\n",
        "      \n",
        "      \n",
        "Y = np.ravel(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-79534cd8061d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfav_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpst_tz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'America/Los_Angeles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pytz' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNsYAEizG0oL",
        "colab_type": "code",
        "outputId": "90b9e551-2dfa-4f58-8f62-fb337612cd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'min_samples_leaf': [1, 2, 4],\n",
        "'min_samples_split': [2, 5, 10],\n",
        "'n_estimators': [200, 400, 600, 800, 1000,\n",
        "1200, 1400, 1600, 1800, 2000]\n",
        "}\n",
        "# Create a based model\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb  = GradientBoostingRegressor(random_state=0)\n",
        "\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = gb, param_grid = param_grid, \n",
        "                         cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
        "\n",
        "grid_search.fit(X_data, Y)\n",
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 10,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 2,\n",
              " 'min_samples_split': 10,\n",
              " 'n_estimators': 1400}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WtiQV9uG0_Y",
        "colab_type": "code",
        "outputId": "6e198415-ba58-4a0e-87b2-cb2eff98a4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#def evaluate(model, test_features, test_labels):\n",
        "    #predictions = model.predict(test_features)\n",
        "    #errors = abs(predictions - test_labels)\n",
        "    #mape = 100 * np.mean(errors / test_labels)\n",
        "    #accuracy = 100 - mape\n",
        "    #print('Model Performance')\n",
        "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    #print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    #return accuracy\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "best_grid = grid_search.best_estimator_\n",
        "print(best_grid)\n",
        "#grid_accuracy = evaluate(best_grid, X_test, y_test)\n",
        "\n",
        "\n",
        "gb =  GradientBoostingRegressor(random_state=0, n_estimators=1400, max_depth=10, max_features='sqrt', \n",
        "                                  min_samples_leaf=2, min_samples_split=10)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Mean Square Error\", metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean-Square Error\" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "# print(\"Accuracy \", metrics.accuracy_score(Y, Y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
            "                          learning_rate=0.1, loss='ls', max_depth=10,\n",
            "                          max_features='sqrt', max_leaf_nodes=None,\n",
            "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                          min_samples_leaf=2, min_samples_split=10,\n",
            "                          min_weight_fraction_leaf=0.0, n_estimators=1400,\n",
            "                          n_iter_no_change=None, presort='auto', random_state=0,\n",
            "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
            "                          verbose=0, warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Square Error 1633763959.6374383\n",
            "Root Mean-Square Error 40419.84611100639\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}